{"origin_pdf_path": "https://www.scitepress.org/Papers/2024/127350/127350.pdf", "text_in_pdf": "A Learning Analytics Dashboard for Improved Learning Outcomes and\nDiversity in Programming Classes\nIris Groher1\na, Michael Vierhauser2\nband Erik Hartl1\n1Johannes Kepler University Linz, Institute of Business Informatics, Software Engineering, Linz, Austria\n2University of Innsbruck, Department of Computer Science, Innsbruck, Austria\nKeywords: Learning Objectives, Assurance of Learning, Learning Analytics, Dashboard, Diversity.\nAbstract: The increased emphasis on competency management and learning objectives in higher education has led to\na rise in Learning Analytics (LA) applications. These tools play a vital role in measuring and optimizing\nlearning outcomes by analyzing and interpreting student-related data. LA tools furthermore provide course\ninstructors with insights on how to refine teaching methods and material and address diversity in student\nperformance to tailor instruction to individual needs. This tool demonstration paper introduces our Learning\nAnalytics Dashboard, designed for an introductory Python programming course. With a focus on gender\ndiversity, the dashboard analyzes graded Jupyter Notebooks, to provide insights into student performance\nacross assignments and exams. An initial assessment of the dashboard, applying it to our Python programming\ncourse in the previous year, has provided us with interesting insights and information on how to further improve\nour class and teaching materials. We present the dashboard’s design, features, and outcomes while outlining\nour plans for its future development and enhancement.\n1 INTRODUCTION\nIn recent years, the systematic management of\ncompetencies and learning objectives has gained\nwidespread popularity, particularly in higher educa-\ntion (Malhotra et al., 2023; Bergsmann et al., 2015).\nIn this context, Learning Analytics (LA) has become\na major endeavor and a means to track and analyze\nlearning outcomes and achievement of competencies.\nLA is primarily concerned with the measurement, col-\nlection, analysis, and interpretation of data related\nto students and their learning in order to understand\nand ultimately optimize learning outcomes (Scheffel\net al., 2014).\nThe importance of applications that support teach-\ning and learning has increased significantly in re-\ncent years, due in no small part to the COVID-19\npandemic. Through the use of technologies such as\nonline platforms, virtual learning environments, and\nlearning management systems (LMS), huge amounts\nof data are generated, which creates opportunities for\nmeasuring the learning success of students and when\nnecessary, positively influencing learning outcomes\na\nhttps://orcid.org/0000-0003-0905-6791\nb\nhttps://orcid.org/0000-0003-2672-9230through targeted intervention (Vieira et al., 2018).\nMaking use of LA can support both students and\neducators in many different ways. For students, it\ncan help to personalize the learning path and enhance\ntheir learning experience. Students can further use LA\nto monitor their own progress and the individual feed-\nback gained can help them understand their strengths\nand weaknesses and make improvements. Educators\ncan use LA to improve the quality of their courses\nand refine their teaching methods, course materials,\nand support services. They can identify difficulties of\ntheir students and potential drop-outs, measure course\nengagement and assessment performance, and mea-\nsure learning objectives to ensure that their courses\nmeet the required standards. LA can further play a\ncrucial role in supporting diversity in higher educa-\ntion by helping educators identify, understand, and\naddress disparities in student performance, engage-\nment, and outcomes.\nAs a step towards LA in Programming Education,\nand to foster diversity analysis, we have created a\nLearning Analytics Dashboard for our introductory\nPython programming course. The dashboard takes\ngraded Jupyter Notebooks (Johnson, 2020) from as-\nsignments or exams as an input, and provides statis-\ntical analyses and visualization of assignments and618\nGroher, I., Vierhauser, M. and Hartl, E.\nA Learning Analytics Dashboard for Improved Learning Outcomes and Diversity in Programming Classes.\nDOI: 10.5220/0012735000003693\nPaper published under CC license (CC BY -NC-ND 4.0)\nInProceedings of the 16th International Conference on Computer Supported Education (CSEDU 2024) - Volume 2 , pages 618-625\nISBN: 978-989-758-697-2; ISSN: 2184-5026\nProceedings Copyright ©2024 by SCITEPRESS – Science and Technology Publications, Lda.\n\ntheir individual exercises or tasks. Furthermore, we\nhave put specific emphasis on the gender diversity\naspect, allowing us to drill down into submissions\nand gain valuable insights into how well certain tasks\nwere performed by different groups of students. Our\nmain goal was, for us as educators and course in-\nstructors, to gain insight into the challenges and dif-\nficulties our students have with the different topics\ncovered in our Python course. As we were facing a\ngender gap, with respect to course performance and\ndrop-outs in the previous semesters, which has also\nbeen frequently reported as a major issue (Marquardt\net al., 2023; Rubio et al., 2015; Groher et al., 2022)\nin computer science classes, we wanted to find out if,\nwhere, and to what extent, female students might face\nincreased difficulties in our course.\nIn this tool demonstration paper, we present our\ninitial version of the dashboard, its application in our\nprogramming course, and the insights and findings we\ngained when using the LA capabilities of the dash-\nboard. We also report on the current and future plans\nto further expand the capabilities of the dashboard.\nThe remainder of this paper is structured as fol-\nlows. In Section 2 we provide a brief introduction\nto the topic of LA in programming education and re-\nlated tools and provide a brief introduction to our in-\ntroductory Python programming course and the main\nrequirements that stem from this course, guiding the\ninitial development of our dashboard. In Section 3\nwe then present the dashboard and its features, with\na concrete application use case in Section 4. Finally,\nin Section 5 we discuss enhancements, additional fea-\ntures we are planning on adding as part of our ongoing\nwork, and conclusions.\n2 BACKGROUND AND COURSE\nSETTING\nIn this section, we present the background and tools\nrelated to our work, and a brief overview of our in-\ntroductory programming course, and requirements for\nour LA dashboard derived from our experiences.\n2.1 Learning Analytics in Programming\nEducation\nLearning analytics has already successfully been ap-\nplied in programming education. L ´opez-Pernas and\nSaqr (L ´opez-Pernas and Saqr, 2021) combine data\nfrom different sources, such as learning manage-\nment systems and programming assessment tools\nto identify learning patterns among students. Theprogramming learning platform Artemis integrates\ncompetency-based learning to generate personal-\nized learning paths for individual students (S ¨olch\net al., 2023). Other work analyzes IDE usage pat-\nterns of students to get insights into their skills\nand performance (Ardimento et al., 2019). Uta-\nmachant et al. (Utamachant et al., 2023) assess stu-\ndent engagement levels and identify at-risk students\nthrough learning activity gaps. In general, LA has\nbeen a growing issue in recent years with active re-\nsearch and a slew of tools on the commercial market.\nMoreover, established LMSs have integrated capabil-\nities into their platforms. For example, Moodle, as\nan open-source platform, provides analytics capabil-\nities via a plug-in extension (Moodle, 2023). Moo-\ndle Analytics provides several different models (static\nand ML-based) that allow generating statistics about,\nfor example, drop-out risks, activities that are due to\nsubmission, and further predictive models. In this\ncontext, Mwalumbwe et al. (Mwalumbwe and Mtebe,\n2017) conducted a study with the intent to develop an\nLA tool and analyze data from Moodle LM systems.\nFocusing on students as a target user group, Peraic\nand Grubisic (Perai ´c and Grubi ˇsi´c, 2022) have pre-\nsented a “Learning Analytics Dashboard for students”\n(LAD-s), providing visualization for student success\nand engagement and further providing predictive an-\nalytics capabilities.\nWoodclap (Woodclap, 2023) is another platform,\nthat focuses on virtual classrooms facilitating com-\nmunication with students on smartphones, messages,\nand real-time interaction while monitoring student en-\ngagements and providing feedback on teaching tech-\nniques. Moreno-Medina et al. (Moreno-Medina et al.,\n2023) used this setting with chemical engineering stu-\ndents in combination with gamification strategies to\nassess and improve student participation and moti-\nvation. Krusche and Berrezueta-Guzman (Krusche\nand Berrezueta-Guzman, 2023) provide an interactive\nlearning environment for programming classes foster-\ning iterative performance enhancement by real-time\nfeedback mechanisms. However, the platform does\nnot integrate support for diversity analysis and is lim-\nited to task-level analysis.\nWhile existing systems offer valuable functionali-\nties for course management and related analysis, they\nlack specific support for assignment-level and task-\nlevel analysis of programming courses. Also, support\nfor diversity analysis is often limited. This motivated\nus to develop a customized dashboard for our setting.A Learning Analytics Dashboard for Improved Learning Outcomes and Diversity in Programming Classes\n619\n\n2.2 Course Setting\nWe started our introductory Python programming\ncourse in 2021, as part of a new university-wide digi-\ntalization initiative, where all study programs (includ-\ning non-technical/CS-related ones) should gain some\nfamiliarity with programming and algorithmic think-\ning. As part of this, we took over the programming\neducation for business students, particularly, business\nadministration and economics.\nWith a total of 6 ECTS, the course is split into\na weekly, slide-based lecture with additional live-\ncoding sessions, and a corresponding weekly exer-\ncise where students should apply the concepts from\nthe previous lecture by solving examples during class\nand as part of their homework. Pair programming\nis applied during the exercise and by this students\nshould work together on programming tasks covering\nthe topic of the lecture. Additionally, homework as-\nsignments consisting of 5-6 individual tasks are dis-\ntributed that have to be completed and submitted by\nthe students within one week. Tutors manually cor-\nrect the assignments, give feedback, and assign points\nto the tasks of the assignments. Students are graded\nbased on the points they receive for the weekly as-\nsignments and an exam at the end of the semester.\nThe main challenge, in this case, was that, com-\npared to a computer science study program, where\none can expect a certain level of technical (and mathe-\nmatical) background, the students participating in our\ncourses are quite diverse, with different educational\nbackgrounds and prior knowledge related to program-\nming. For most students, our course was the first\ntime they have written code and/or executed a pro-\ngram written by themselves.\nFor this purpose, we opted for Python as a pro-\ngramming language, instead of Java – which is the\nstandard language for programming education in CS\ncourses, in conjunction with Jupyter Notebooks. The\nweekly assignments are distributed as Jupyter Note-\nbooks and the students submit their solutions as note-\nbooks in Moodle. The final exam at the end of the\nsemester is conducted with the CodeRunner (Lobb,\nRichard and Hunt, Tim, 2023) plugin in Moodle.\n2.3 Stakeholders & Requirements\nIn higher education, effective utilization of LA plays\nan important role in ensuring effective curriculum\nmanagement and enhancing student outcomes. This\nis especially important for lecturers , and course in-\nstructors who engage directly with the course content\nand the students. While the initial design of our plat-\nform primarily targets the needs of these educators,there’s also a foresight to expand the platform’s capa-\nbilities to incorporate the needs of students and pro-\ngram managers in the future. For now, we defined the\nfollowing requirements for our dashboard:\nR1. Course Management. A fundamental re-\nquirement revolves around the management of\ncourses. This includes functionalities to set\ncourse settings, such as determining the start\nand end dates, entering the number of assign-\nments, defining requirements such as the num-\nber of submissions and points for passing the\ncourse, and setting the number of students en-\nrolled in the course.\nR2. Document Management. The dashboard\nshould allow for the seamless upload of graded\nJupyter Notebooks, adhering to a defined JSON\nformat.\nR3. Analytical Insight into Assignments. To track\ncourse progress and ensure equitable assess-\nment, there’s a need to provide analytics about\nthe number of submissions per assignment over\nthe semester.\nR4. Student Data Management. This encompasses\nthe ability to manage pertinent student data, in-\ncluding their names, IDs, gender, and details\nabout their enrolled study program.\nR5. Descriptive Statistics on Performance. For an\nin-depth analysis of student performance, edu-\ncators require a distribution of points per assign-\nment over the semester for all students. Addi-\ntionally, a separate analysis filtered by gender,\nvisualized using box plots to depict the variabil-\nity and central tendency is needed. For each as-\nsignment a detailed breakdown into the number\nof submissions, average points for the collective\nstudent body, and an analysis separated by gen-\nder is necessary. An average effort metric fur-\nther illuminates the student’s engagement levels.\nA similar granularity of insights is required for\neach task and exam.\nR6. Individual Student Analytics. For personal-\nized feedback and support, each student’s pro-\nfile should be enriched with their performance\nmetrics, including points per assignment, aver-\nage points, pass status, number of submissions,\nand other relevant details.\nR7. Export Capability. Recognizing the diverse\nuses of such data, there should be a provision for\nexporting student-specific data for further anal-\nysis or reporting purposes.CSEDU 2024 - 16th International Conference on Computer Supported Education\n620\n\nFigure 1: Learning Analytics Dashboard: Assignment-level analysis view.\n3 ANALYSIS DASHBOARD\nIn this section, we present details of our Learning An-\nalytics Dashboard by first providing an overview of its\nmain features from this first iteration prototype and\nthen providing a brief overview of the technical de-\ntails of the implementation. A short demonstration\nvideo of the main Dashboard features1is available on-\nline.\n3.1 Features\nThe requirements of our initial implementation were\ndriven by the need to gain insights into students’ abil-\nity to successfully solve assignments, the submission\nrates of individual assignments and constituent tasks,\nand identifying potential gender gaps (cf. R3, R5),\nhence largely by our own requirements. In the fol-\nlowing, we provide a brief overview of the function-\nality and show examples in the dashboard (cf. Fig. 1\nand Fig. 2 )\n•General Analysis and Trends of Assignments. In\nour previous programming classes, we have often ex-\nperienced drop-outs in the middle of the semester or\neven towards the end of the course. Therefore, one of\nour main requirements was to get a better overview\nof individual assignments (handed out on a weekly\nbasis), and whether there was a steady number of\nhanded-in assignments (and successfully completed\n1https://github.com/TeachingAndLearningSciences/res\nourcestasks within the assignment) or a noticeable decline in\nsubmissions over time. Fig. 1 provides an overview of\nthe main view of the dashboard. The top part [A] pro-\nvides information about the raw submission numbers\nfor each assignment and allows us to easily identify\nif submission numbers are declining for a particular\nassignment, or steadily over time. Additionally, the\nlower [B] part provides an overview of the results for\neach assignment, i.e. the points achieved by students,\nand the spectrum (Box Plot) of the results. This helps\nus to identify exercises that might be particularly dif-\nficult (where students have received fewer points) or\npotential effects of different educational backgrounds\n(where we have a broad spectrum of points achieved).\n•Detailed Analysis on Assignment and Task Level.\nWhile analysis on the assignment level can provide\nsome valuable insights into the overall course, it does\nnot provide sufficient details on how students handle\nindividual assignments and the topics and ultimately\nlearning objectives associated with these assignments\n(and the constituent tasks). The second analysis level\n(cf. Fig. 2), therefore, is concerned with drilling down\ninto individual assignments and tasks part of the as-\nsignment (cf. R6). The top part [C] in this case pro-\nvides again an overview of submission numbers and\nresults, whereas the bottom part [D] goes into further\ndetail for each of the tasks. For each task, we get de-\ntailed insights into how well the students performed,\nin terms of the points achieved. The ”point-per-point”\nvisualization (Grouped Bar Charts) provides detailed\ninsights on the distribution of points for individual\ntasks and allows us to identify tasks that might be po-A Learning Analytics Dashboard for Improved Learning Outcomes and Diversity in Programming Classes\n621\n\nFigure 2: Learning Analytics Dashboard: Task-level analysis view.\ntentially too difficult or complex.\n•Gender Analysis. A cross-cutting concern for all\nanalysis activities is the aspect of gender. Based on\nour previous, multi-year, experience of offering basic\nprogramming classes for various different study pro-\ngrams, we have observed gender gaps in several of our\ncourses. Research in this area has shown that precau-\ntionary measures and actions can (at least partially)\nrectify such issues, for example, by providing ap-\npropriate teaching material and assignments (Schmitz\nand Nikoleyczik, 2009; Spieler and Slany, 2018).\n•Automated Analysis. One of our key requirements\nwas to facilitate automated analysis of graded note-\nbooks, while retaining manual grading of assignments\nperformed manually by tutors. Tutors do not only\ngrade assignments and tasks, but provide individual\nfeedback about how well a problem was solved, and\ngive hints and samples when a task could not be com-\npleted. With weekly assignments, the workload for\ntutors is already quite high and we did not want to bur-\nden them with additional requirements (e.g., entering\nresults in yet another tool – i.e. our dashboard). In-\nstead, we opted for an automated parser, that reads out\nassignment/task points (which are entered in a struc-\ntured manner) and stores them as JSON information\nin the meta-data of the notebook (cf. R2). This in-\nformation is then used for subsequent analysis in the\ndashboard. As a positive side-effect, this also decou-\nples the dashboard from the specific structure/format\nof the assignments and allows for updated/changed\nJupyter notebooks in the future, as long as the grading\ninformation is provided in the predefined JSON for-\nmat. This further contributes to the aspect of general-\nizability of the dashboard with potential applications\nto other (types) of programming classes (cf. further\ndiscussion in Section 5).\n•Other Capabilities. Besides the main analysiscapabilities, additional functionality is related to the\nability to define courses with respective course set-\ntings (e.g., the number of students part of the course,\ngrading schemes, and number of assignments) (cf.\nR1). Additionally, as establishing interfaces to exist-\ning university systems where student data is stored,\nis typically challenging, we added the ability to store\nbasic student information (e.g., names, gender, study\nprogram), to ensure data privacy, stored only locally\non university premises (cf. R4). In conjunction with\nthis, we also added the ability to export results (cf.\nR7) in a standard CSV format, to enable grading in-\nformation to be fed back to the existing university\ngrading system.\n3.2 Implementation\nTo facilitate easy access and availability to a broad\nrange of users, we decided to implement our Learn-\ning Analytics Dashboard as a web application using\nJavaScript. The components of the dashboard are\nstructured in a 3-tier architecture, presentation, logic,\nand data layer with central data storage. For this pur-\npose, we use a PostgreSQL database where informa-\ntion in courses, and results extracted from the Jupyter\nnotebooks are stored. As the dashboard uses sensi-\nble data concerning students and learning outcomes,\nwe refrained from using cloud services, but deployed\nour application as containers using Docker2. The core\nimplementation, presentation, and logic use Node.js3,\nNext.js4, and React5.\n2https://www.docker.com\n3https://nodejs.org/en\n4https://nextjs.org\n5https://react.devCSEDU 2024 - 16th International Conference on Computer Supported Education\n622\n\n4 APPLICATION EXAMPLE &\nDISCUSSION\nAs an initial assessment of the usefulness of our\nLearning Analytics Dashboard application, we used\nit to analyze one iteration of our Python course in the\nsummer semester of 2022. In this section, we present\nthe insights and findings and further discuss its limi-\ntations and potential threats that need to be taken into\naccount.\n4.1 Analyzed Python Course\nAs part of this initial validation, we used the\ndashboard to analyze students’ performance in 10\nhomework programming assignments throughout the\ncourse. The assignments covered a range of topics\nfrom variables and data types, to advanced modules\nlike NumPy and Pandas, as well as object orientation.\nFig. 3 (top) shows the number of submissions for\neach assignment and the distribution of points re-\nceived for all students. The bottom part shows the\ndetailed results for two tasks part of Assignment 9.\nThe analysis of the assignments with the help of our\ndashboard revealed several key insights.\n•Submission Trends. While there was a slight de-\ncrease in the number of submissions for Assignments\n9 and 10, we did not observe a significant drop-out.\nThe lower numbers for Assignments 9 and 10 can be\nattributed to the fact that only 8 out of 10 submissions\nwere mandatory.\n•Assignment Metrics. For the first three assign-\nments, covering basic concepts, data types, and sim-\nple programs, we observed a high average score and\nlittle spread, However, further into the semester and\nwith increasing complexity, from Assignment 4 on-\nward, we observed a lower average score and a higher\nspread in points.\n•Assignment Drill-Down. The ability to further an-\nalyze constituent tasks of an assignment also revealed\nsome interesting insights, particularly for assignments\nwhere we already observed a significant spread in\npoints. Particularly, for Assignment 9, which covered\nthe topic of modules including Math, NumPy, Mat-\nplotlib, and Pandas, we observed the largest variabil-\nity in scores. Notably, one-third of the students scored\nless than 1 out of 3 points in the pandas task, whereas\nhalf of the students reached the maximum score of 3\n(cf. Fig. 3 – bottom part). For Assignment 10, which\nfocused on object orientations, the median score in-\ncreased, indicating better understanding compared to\nthe previous assignment.\n•Gender Gap. Throughout the semester and across\nall assignments, in contrast to our initial assumptions,\nMatplotlib\nPandasFigure 3: Results from our analysis – Trends in Assignment\nSubmission/Points (top) and detailed results for tasks (Pan-\ndas and Matplotlib) in the modules assignment (bottom).\nno significant gender-based performance gap was de-\ntected. We explicitly designed this course in an inclu-\nsive way based on our previous findings in an intro-\nductory Java course. Further analysis of exam results\nand time spent on homework assignments, however,\nare required to further confirm the gender equality in\nour Python course.\n4.2 Implications and Limitations\nUsing the visualization capabilities of the dashboard\nwe were able to identify issues during the semester\npertaining to assignments and the topics covered in\nthe class. Parts of these findings were later – at the\nend of the semester – also confirmed through a ques-\ntionnaire we sent out to students, where we specif-\nically asked for issues/challenges they experienced,\nand improvements for the class. Assignment 9, cov-\nering different modules like Pandas and Matplotlib,\nseemed to be particularly difficult for the students in\nour course. We, therefore, developed additional ma-\nterial and planned an extra lesson in the following\nsemester. Also, the rule that students only need to\nsubmit 8 out of 10 exercises leads to the fact that\nmany students drop the last two exercises (covering\nmodules and object orientation). As a result, we planA Learning Analytics Dashboard for Improved Learning Outcomes and Diversity in Programming Classes\n623\n\nto change this rule in future semesters especially the\ntopic of modules is needed in courses of subsequent\nsemesters.\n•Limitations. This preliminary validation does not\ncapture other potential factors affecting student per-\nformance, such as attendance, participation in tuto-\nrials, or specific educational backgrounds hence, we\ncan only draw limited conclusions about the learning\noutcomes of the course. However, the primary pur-\npose was to assess the usefulness of our dashboard\nand the initial set of visualizations and statistical anal-\nyses that are provided. Furthermore, we so far only\ncovered one semester, but after initial positive results,\nour future plans to extend and apply the dashboard to\nsubsequent iterations and other programming classes\n(cf. Section 5) will provide us with additional data and\nrelevant stakeholders for our tool.\n4.3 Discussion\nWhile our analytics dashboard offers many possibil-\nities for enhancing Python programming education,\nit also raises several concerns that require attention.\nOne of the most important issues regarding the im-\nplementation of our Learning Analytics Dashboard is\nthe concern for student privacy. The dashboard col-\nlects and analyzes various types of data, including as-\nsignment points, task points, and gender information.\nWhile this data is important for educational insights,\nit also raises questions about the confidentiality and\nanonymity of student information. Ensuring that the\ndata is securely stored and accessed only by autho-\nrized personnel is vital. Additionally, the dashboard\nmust comply with relevant data protection regulations\nto ensure student privacy.\nEthical considerations extend beyond data pri-\nvacy. The gender analysis feature, for instance, could\ninadvertently preserve stereotypes or biases if not\ncarefully designed and interpreted. There is also the\nethical question of how the data should be used. For\nexample, should low performance of students trigger\nan automatic alert to educational staff, or should the\ndata only serve as an analytical tool for course im-\nprovement? Balancing data utility and ethical consid-\nerations is crucial in this case.\nThe risk of data misinterpretation is inherent in\nany analytics tool. In the educational context, incor-\nrect interpretation of the dashboard’s data could lead\nto misplaced educational interventions. For exam-\nple, a gender-based performance gap in assignment\npoints might be wrongly attributed to pedagogical is-\nsues when external factors could be influencing the\ndata. Therefore, it is essential to provide adequate\ntraining for lecturers and program managers who willbe interpreting the dashboard’s data. Contextualizing\nthe data with qualitative insights is also recommended\nto avoid simplistic or misleading conclusions.\nFuture work should focus on addressing these is-\nsues through a combination of technical safeguards,\npolicies, and user education to ensure that the dash-\nboard serves as an effective, ethical, and secure edu-\ncational tool.\n5 CONCLUSION AND FUTURE\nWORK\nThe rapid development of LA in higher education em-\nphasizes the need for systematic management of com-\npetencies and learning objectives. In this tool demon-\nstration paper, we introduced an innovative Learning\nAnalytics Dashboard specifically designed for an in-\ntroductory Python programming course. This dash-\nboard not only aims to assist educators in pedagogi-\ncal decisions but also focuses on the critical area of\ngender diversity within the course setting.\nOur initial application which we used for our own\nanalysis, provided a series of valuable insights into\nstudent performance and engagement, pointing out\nspecific challenges regarding the topics of modules\nin Python. We could not detect a significant gender\ngap and drop-out rates in our course. These insights,\neven with our initial prototype, already demonstrated\nthe power of LA as not just a reactive tool for un-\nderstanding student performance, but as a proactive\nmechanism that allows for targeted interventions to\nenhance educational equality.\nFuture work will expand on these initial successes.\nWe plan to enhance the dashboard’s capabilities to\ninclude more diversified analytics features, poten-\ntially adding support for analyzing different educa-\ntional backgrounds. We further plan to add support\nfor competency management and the establishment\nof links between competencies and assignment and\nexam tasks and to analyze competency coverage of\nthe tasks and competency achievements of students\nin the course. We are currently also working on sup-\nport to increase the degree of automation. This in-\ncludes a dedicated grading-support plug-in in Visual-\nStudio for tutors, that generates the JSON data and\nautomatically uploads notebooks to the dashboard\nwhen graded. Furthermore, we plan on going beyond\nJupyter notebook-based Python courses, and support\nfor analysis capabilities over multiple semesters. The\ncurrent version of our dashboard focuses on educa-\ntors as our primary stakeholders. In the future, we\nwill also provide views for students to monitor their\nprogress in the course.CSEDU 2024 - 16th International Conference on Computer Supported Education\n624\n\nREFERENCES\nArdimento, P., Bernardi, M. L., Cimitile, M., and Ruvo,\nG. D. (2019). Learning analytics to improve coding\nabilities: a fuzzy-based process mining approach. In\nProc. of the 2019 IEEE International Conference on\nFuzzy Systems , pages 1–7. IEEE.\nBergsmann, E., Schultes, M.-T., Winter, P., Schober, B.,\nand Spiel, C. (2015). Evaluation of competence-based\nteaching in higher education: From theory to practice.\nEvaluation and Program Planning , 52:1–9.\nGroher, I., Vierhauser, M., Sabitzer, B., Kuka, L., Hofer, A.,\nand Muster, D. (2022). Exploring diversity in intro-\nductory programming classes: an experience report.\nInProc. of the ACM/IEEE 44th International Confer-\nence on Software Engineering: Software Engineering\nEducation and Training , pages 102–112. IEEE.\nJohnson, J. W. (2020). Benefits and pitfalls of jupyter\nnotebooks in the classroom. In Proc. of the 21st an-\nnual Conference on Information Technology Educa-\ntion, pages 32–37. ACM.\nKrusche, S. and Berrezueta-Guzman, J. (2023). Introduc-\ntion to programming using interactive learning. In\nProc. of the 2023 IEEE 35th International Confer-\nence on Software Engineering Education and Train-\ning, pages 178–182. IEEE.\nLobb, Richard and Hunt, Tim (2023). Moodle CodeRun-\nner. https://moodle.org/plugins/qtype coderunner.\n[Online; Accessed 01-10-2023].\nL´opez-Pernas, S. and Saqr, M. (2021). Bringing synchrony\nand clarity to complex multi-channel data: A learn-\ning analytics study in programming education. IEEE\nAccess , 9:166531–166541.\nMalhotra, R., Massoudi, M., and Jindal, R. (2023). Shift-\ning from traditional engineering education towards\ncompetency-based approach: The most recommended\napproach-review. Education and Information Tech-\nnologies , 28(7):9081–9111.\nMarquardt, K., Wagner, I., and Happe, L. (2023). Engag-\ning girls in computer science: Do single-gender inter-\ndisciplinary classes help? In 2023 IEEE/ACM 45th\nInternational Conference on Software Engineering:\nSoftware Engineering Education and Training (ICSE-\nSEET) , pages 128–140. IEEE.\nMoodle (2023). Moodle Analytics. https://docs.moodle.or\ng/402/en/Analytics. [Online; Accessed 01-10-2023].\nMoreno-Medina, I., Pe ˜nas-Garz ´on, M., Belver, C., and\nBedia, J. (2023). Wooclap for improving student\nachievement and motivation in the chemical engi-\nneering degree. Education for Chemical Engineers ,\n45:11–18.\nMwalumbwe, I. and Mtebe, J. S. (2017). Using learning\nanalytics to predict students’ performance in moodle\nlearning management system: A case of mbeya uni-\nversity of science and technology. The Electronic\nJournal of Information Systems in Developing Coun-\ntries, 79(1):1–13.\nPerai ´c, I. and Grubi ˇsi´c, A. (2022). Development and eval-\nuation of a learning analytics dashboard for moodle\nlearning management system. In Proc. of the 2022HCI International Conference - Late Breaking Pa-\npers. Interaction in New Media, Learning and Games ,\npages 390–408, Cham. Springer Nature Switzerland.\nRubio, M. A., Romero-Zaliz, R., Ma ˜noso, C., and\nde Madrid, A. P. (2015). Closing the gender gap in\nan introductory programming course. Computers &\nEducation , 82:409–420.\nScheffel, M., Drachsler, H., Stoyanov, S., and Specht, M.\n(2014). Quality indicators for learning analytics. Jour-\nnal of Educational Technology & Society , 17(4):117–\n132.\nSchmitz, S. and Nikoleyczik, K. (2009). Transdisciplinary\nand gender-sensitive teaching: didactical concepts and\ntechnical support. International Journal of Innovation\nin Education , 1.\nSpieler, B. and Slany, W. (2018). Female teenagers and\ncoding: Create gender sensitive and creative learning\nenvironments. In Constructionism 2018: Construc-\ntionism, Computational Thinking and Educational In-\nnovation , pages 405–414.\nS¨olch, M., Aberle, M., and Krusche, S. (2023). Integrat-\ning competency-based education in interactive learn-\ning systems. In Companion Proc. of the 13th Interna-\ntional Learning Analytics and Knowledge Conference ,\npages 53–56.\nUtamachant, P., Anutariya, C., and Pongnumkul, S. (2023).\ni-ntervene: applying an evidence-based learning ana-\nlytics intervention to support computer programming\ninstruction. Smart Learning Environments , 10:37.\nVieira, C., Parsons, P., and Byrd, V . (2018). Visual learning\nanalytics of educational data: A systematic literature\nreview and research agenda. Computers & Education ,\n122:119–135.\nWoodclap (2023). Woodclap. https://www.wooclap.com/\nde/. [Online; Accessed 01-10-2023].A Learning Analytics Dashboard for Improved Learning Outcomes and Diversity in Programming Classes\n625", "files_in_pdf": []}